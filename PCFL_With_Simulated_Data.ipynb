{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "-------------\n",
    "\n",
    "This notebook carries out the experiment on simulated data in Sect. 6.1 of our paper \"Pragmatic Causal Feature Learning\". The experiment proceeds in two stages, first deriving a coarsening using Chalupka et al.'s CFL method, before using the same data to derive a different coarsening using our PCFL method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This cell sets up the Python environment. Make sure that you have the following packages installed:\n",
    "numpy\n",
    "sklearn\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1423)\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the number of data points and the range of C and E. Note that some steps can take a while when m>10000, and that our code is prohibitively slow at m=1000000. \n",
    "m=10000\n",
    "R_C = np.array([2,4,6,8])\n",
    "R_E = np.array([2,4,6,8])\n",
    "\n",
    "#Generate data using the joint probability distribution over CxE.\n",
    "pairs = np.array(np.transpose([np.tile(R_C, len(R_E)), np.repeat(R_E, len(R_C))]))\n",
    "probs = np.array([1/20,1/40,97/400,1/20,1/40,3/80,1/343,1/40,1/40,3/80,1/343,1/40,3/20,3/20,1/600,3/20])\n",
    "probs[15] = probs[15]+(1-sum(probs))\n",
    "data_indeces = np.array([np.random.choice(np.arange(0,16),p=probs) for i in range(0,m)])\n",
    "data = np.array([pairs[data_indeces[i]] for i in range(0,m)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X and Y vectors based on the data set.\n",
    "\n",
    "X = np.array([data[i][0] for i in range(0,m)]).reshape(-1,1)\n",
    "Y = np.array([data[i][1] for i in range(0,m)]).reshape(-1,1)\n",
    "\n",
    "#Define a utility function over XxY, and record utilities in a vector Z.\n",
    "def u(x,y):\n",
    "    if y!=8:\n",
    "        return 100-x*y\n",
    "    else:\n",
    "        return 100-(x*(y-2))\n",
    "    \n",
    "Z = np.array([u(data[i][0],data[i][1]) for i in range(0,m)]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFL\n",
    "-------------\n",
    "\n",
    "The next five cells implement Chalupka et al.'s CFL algorithm on the data generated above (Alg. 1 in our paper) to learn a three-valued coarsening of the cause and effect variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regress X on Y using cubic regression. \n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=3)),('linear', LinearRegression(fit_intercept=False))])\n",
    "model = model.fit(X, Y)\n",
    "X_estimator = np.array([model.named_steps['linear'].coef_[0][0] + model.named_steps['linear'].coef_[0][1]*X[i] + model.named_steps['linear'].coef_[0][2]*X[i]**2 + model.named_steps['linear'].coef_[0][3]*X[i]**3 for i in range(0,m)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster Xs.\n",
    "N_CLASSES = 3\n",
    "X_lbls = KMeans(n_clusters=N_CLASSES, n_init=10, n_jobs=-1).fit_predict(X_estimator.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing P(y | X_lbls) features, iter 9900/10000...Done. Clustering P(y | X_lbls).\n"
     ]
    }
   ],
   "source": [
    "#Cluster Ys.\n",
    "Y_ftrs = np.zeros((Y.shape[0], np.unique(X_lbls).size))\n",
    "# Loop, not vectorized, to save memory. Can take a while.\n",
    "for Y_id, y in enumerate(Y):\n",
    "    if Y_id % 100==0:\n",
    "        sys.stdout.write('\\rComputing P(y | X_lbls) features, iter {}/{}...'.format(Y_id, Y.shape[0]))\n",
    "        sys.stdout.flush() \n",
    "    for X_lbl_id, X_lbl in enumerate(np.unique(X_lbls)):\n",
    "        # Find ids of xs in this X_lbls class.\n",
    "        X_lbl_ids = np.where(X_lbls==X_lbl)[0]\n",
    "        # Compute distances of y to all y's in this X_lbls class and sort them.\n",
    "        sorted_dists = np.sort([np.sum((y-Y[X_lbl_ids[i]])**2) for i in range(0,len(X_lbl_ids))])\n",
    "        # Find the mean distance to the 4 closest non-zero points.\n",
    "        Non_Zero_Distance = np.where(sorted_dists!=0)[0]\n",
    "        Y_ftrs[Y_id][X_lbl_id] = Non_Zero_Distance[1:5].mean()\n",
    "print('Done. Clustering P(y | X_lbls).')\n",
    "Y_lbls = KMeans(n_clusters=N_CLASSES, n_init=10, n_jobs=-1).fit_predict(Y_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create labels showing how variables have been coarsened.\n",
    "\n",
    "full_matrix=np.hstack((data,X_lbls.reshape(-1,1),Y_lbls.reshape(-1,1)))\n",
    "\n",
    "crs_C0 = np.delete(full_matrix,np.where(full_matrix[:,2]!=0),axis=0)\n",
    "crs_C0 = crs_C0[:,0].reshape(-1,1)\n",
    "crs_C0 = np.array([crs_C0[i] for i in range(0,len(crs_C0))])\n",
    "crs_C0 = list(np.unique(crs_C0))\n",
    "crs_C1 = np.delete(full_matrix,np.where(full_matrix[:,2]!=1),axis=0)\n",
    "crs_C1 = crs_C1[:,0].reshape(-1,1)\n",
    "crs_C1 = np.array([crs_C1[i] for i in range(0,len(crs_C1))])\n",
    "crs_C1 = list(np.unique(crs_C1))\n",
    "crs_C2 = np.delete(full_matrix,np.where(full_matrix[:,2]!=2),axis=0)\n",
    "crs_C2 = crs_C2[:,0].reshape(-1,1)\n",
    "crs_C2 = np.array([crs_C2[i] for i in range(0,len(crs_C2))])\n",
    "crs_C2 = list(np.unique(crs_C2))\n",
    "\n",
    "crs_E0 = np.delete(full_matrix,np.where(full_matrix[:,3]!=0),axis=0)\n",
    "crs_E0 = crs_E0[:,1].reshape(-1,1)\n",
    "crs_E0 = np.array([crs_E0[i] for i in range(0,len(crs_E0))])\n",
    "crs_E0 = list(np.unique(crs_E0))\n",
    "crs_E1 = np.delete(full_matrix,np.where(full_matrix[:,3]!=1),axis=0)\n",
    "crs_E1 = crs_E1[:,1].reshape(-1,1)\n",
    "crs_E1 = np.array([crs_E1[i] for i in range(0,len(crs_E1))])\n",
    "crs_E1 = list(np.unique(crs_E1))\n",
    "crs_E2 = np.delete(full_matrix,np.where(full_matrix[:,3]!=2),axis=0)\n",
    "crs_E2 = crs_E2[:,1].reshape(-1,1)\n",
    "crs_E2 = np.array([crs_E2[i] for i in range(0,len(crs_E2))])\n",
    "crs_E2 = list(np.unique(crs_E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['    ' 'crs_E0' 'crs_E1' 'crs_E2']\n",
      " ['crs_C0' '0.6' '0.11' '0.3']\n",
      " ['crs_C1' '0.01' '0.97' '0.02']\n",
      " ['crs_C2' '0.6' '0.21' '0.19']]\n",
      "crs_C0= [4]\n",
      "crs_C1= [6]\n",
      "crs_C2= [2, 8]\n",
      "crs_E0= [8]\n",
      "crs_E1= [2]\n",
      "crs_E2= [4, 6]\n"
     ]
    }
   ],
   "source": [
    "#Find the conditional probability table and print, with labels. Note the guide after the table showing which fine-grained variable values have been coarsened together. \n",
    "\n",
    "P_CE = np.array([np.bincount(Y_lbls.astype(int)[X_lbls==X_lbl],minlength=Y_lbls.max()+1).astype(float) for X_lbl in np.sort(np.unique(X_lbls))])\n",
    "P_CE = P_CE/P_CE.sum()\n",
    "P_E_given_C = np.around(P_CE/P_CE.sum(axis=1, keepdims=True),2)\n",
    "column = np.array(['crs_C0','crs_C1','crs_C2']).reshape(-1,1)\n",
    "header = np.array(['    ','crs_E0','crs_E1','crs_E2'])\n",
    "P_E_given_C = np.hstack((column,P_E_given_C))\n",
    "P_E_given_C = np.vstack((header,P_E_given_C))\n",
    "print(P_E_given_C)\n",
    "print('crs_C0=', crs_C0)\n",
    "print('crs_C1=', crs_C1)\n",
    "print('crs_C2=', crs_C2)\n",
    "print('crs_E0=', crs_E0)\n",
    "print('crs_E1=', crs_E1)\n",
    "print('crs_E2=', crs_E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCFL\n",
    "-------------\n",
    "\n",
    "The next five cells implement our PCFL algorithm on the data generated above (Alg. 2 in our paper) to learn a three-valued pragmatic coarsening of the cause and effect variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regress X on the vector Z of utility values u(x,y)\n",
    "\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=3)),('linear', LinearRegression(fit_intercept=False))])\n",
    "model = model.fit(X, Z)\n",
    "X_estimator_prag = np.array([model.named_steps['linear'].coef_[0][0] + model.named_steps['linear'].coef_[0][1]*X[i] + model.named_steps['linear'].coef_[0][2]*X[i]**2 + model.named_steps['linear'].coef_[0][3]*X[i]**3 for i in range(0,m)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Xs.\n",
    "N_CLASSES = 3\n",
    "X_lbls_prag = KMeans(n_clusters=N_CLASSES, n_init=10, n_jobs=-1).fit_predict(X_estimator_prag.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Clustering P(y | X_lbls).\n"
     ]
    }
   ],
   "source": [
    "#Cluster Ys. Takes less time than this step does when implementing CFL. \n",
    "Y_ftrs = np.zeros((Y.shape[0],m))\n",
    "for i in range(0,m):\n",
    "        helper_matrix = np.hstack((data,Z))\n",
    "        helper_matrix = np.delete(helper_matrix,np.where(helper_matrix[:,1]!=Y[i]),axis=0)\n",
    "        Y_ftrs[i] = helper_matrix[:,2].mean()\n",
    "print('Done. Clustering P(y | X_lbls).')\n",
    "Y_lbls_prag = KMeans(n_clusters=N_CLASSES, n_init=10, n_jobs=-1).fit_predict(Y_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create labels showing how variables have been coarsened.\n",
    "\n",
    "full_matrix=np.hstack((data,X_lbls_prag.reshape(-1,1),Y_lbls_prag.reshape(-1,1)))\n",
    "\n",
    "crs_C0 = np.delete(full_matrix,np.where(full_matrix[:,2]!=0),axis=0)\n",
    "crs_C0 = crs_C0[:,0].reshape(-1,1)\n",
    "crs_C0 = np.array([crs_C0[i] for i in range(0,len(crs_C0))])\n",
    "crs_C0 = list(np.unique(crs_C0))\n",
    "crs_C1 = np.delete(full_matrix,np.where(full_matrix[:,2]!=1),axis=0)\n",
    "crs_C1 = crs_C1[:,0].reshape(-1,1)\n",
    "crs_C1 = np.array([crs_C1[i] for i in range(0,len(crs_C1))])\n",
    "crs_C1 = list(np.unique(crs_C1))\n",
    "crs_C2 = np.delete(full_matrix,np.where(full_matrix[:,2]!=2),axis=0)\n",
    "crs_C2 = crs_C2[:,0].reshape(-1,1)\n",
    "crs_C2 = np.array([crs_C2[i] for i in range(0,len(crs_C2))])\n",
    "crs_C2 = list(np.unique(crs_C2))\n",
    "\n",
    "crs_E0 = np.delete(full_matrix,np.where(full_matrix[:,3]!=0),axis=0)\n",
    "crs_E0 = crs_E0[:,1].reshape(-1,1)\n",
    "crs_E0 = np.array([crs_E0[i] for i in range(0,len(crs_E0))])\n",
    "crs_E0 = list(np.unique(crs_E0))\n",
    "crs_E1 = np.delete(full_matrix,np.where(full_matrix[:,3]!=1),axis=0)\n",
    "crs_E1 = crs_E1[:,1].reshape(-1,1)\n",
    "crs_E1 = np.array([crs_E1[i] for i in range(0,len(crs_E1))])\n",
    "crs_E1 = list(np.unique(crs_E1))\n",
    "crs_E2 = np.delete(full_matrix,np.where(full_matrix[:,3]!=2),axis=0)\n",
    "crs_E2 = crs_E2[:,1].reshape(-1,1)\n",
    "crs_E2 = np.array([crs_E2[i] for i in range(0,len(crs_E2))])\n",
    "crs_E2 = list(np.unique(crs_E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['    ' 'crs_E0' 'crs_E1' 'crs_E2']\n",
      " ['crs_C0' '0.59' '0.36' '0.05']\n",
      " ['crs_C1' '0.21' '0.69' '0.1']\n",
      " ['crs_C2' '0.11' '0.74' '0.15']]\n",
      "crs_C0= [2, 6]\n",
      "crs_C1= [8]\n",
      "crs_C2= [4]\n",
      "crs_E0= [2]\n",
      "crs_E1= [6, 8]\n",
      "crs_E2= [4]\n"
     ]
    }
   ],
   "source": [
    "#Find the conditional probability table and print, with labels. Note the guide after the table showing which fine-grained variable values have been coarsened together. \n",
    "P_CE = np.array([np.bincount(Y_lbls_prag.astype(int)[X_lbls_prag==X_lbl],minlength=Y_lbls_prag.max()+1).astype(float) for X_lbl in np.sort(np.unique(X_lbls_prag))])\n",
    "P_CE = P_CE/P_CE.sum()\n",
    "P_E_given_C = np.around(P_CE/P_CE.sum(axis=1, keepdims=True),2)\n",
    "column = np.array(['crs_C0','crs_C1','crs_C2']).reshape(-1,1)\n",
    "header = np.array(['    ','crs_E0','crs_E1','crs_E2'])\n",
    "P_E_given_C = np.hstack((column,P_E_given_C))\n",
    "P_E_given_C = np.vstack((header,P_E_given_C))\n",
    "print(P_E_given_C)\n",
    "print('crs_C0=', crs_C0)\n",
    "print('crs_C1=', crs_C1)\n",
    "print('crs_C2=', crs_C2)\n",
    "print('crs_E0=', crs_E0)\n",
    "print('crs_E1=', crs_E1)\n",
    "print('crs_E2=', crs_E2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
